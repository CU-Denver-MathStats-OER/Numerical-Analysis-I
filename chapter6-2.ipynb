{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64730ae5",
   "metadata": {},
   "source": [
    "# 6.2 Continuous least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba069f5",
   "metadata": {},
   "source": [
    "In discrete least squares, our starting point was a set of data points.\n",
    "Here we will start with a continuous function $f$ on $[a,b]$ and\n",
    "answer the following question: how can we find the \"best\" polynomial\n",
    "$P_{n}(x)=\\sum_{j=0}^{n}a_{j}x^{j}$ of degree at most $n$ that\n",
    "approximates $f$ on $[a,b]?$ As before, \"best\" polynomial will mean the\n",
    "polynomial that minimizes the least squares error:\n",
    "\n",
    "\\begin{equation}\\label{eq:least_sq_error}\n",
    "E=\\int_{a}^{b}\\left(f(x)-\\sum_{j=0}^{n}a_{j}x^{j}\\right)^{2}dx.\n",
    "\\end{equation}\n",
    "\n",
    "Compare this expression with that of the **discrete least squares**:\n",
    "\\begin{equation*}\n",
    "E=\\sum_{i=1}^{m}\\left(y_{i}-\\sum_{j=0}^{n}a_{j}x_{i}^{j}\\right)^{2}.\n",
    "\\end{equation*}\n",
    "To minimize $E$ in \\eqref{eq:least_sq_error} we set $\\frac{\\partial E}{\\partial a_{k}}=0$, for\n",
    "$k=0,1,...,n,$ and observe\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial a_{k}} & =\\frac{\\partial}{\\partial a_{k}}\\left(\\int_{a}^{b}f^{2}(x)dx-2\\int_{a}^{b}f(x)\\left(\\sum_{j=0}^{n}a_{j}x^{j}\\right)dx+\\int_{a}^{b}\\left(\\sum_{j=0}^{n}a_{j}x^{j}\\right)^{2}dx\\right)\\\\\n",
    " & =-2\\int_{a}^{b}f(x)x^{k}dx+2\\sum_{j=0}^na_{j}\\int_{a}^{b}x^{j+k}dx=0,\n",
    "\\end{align*}\n",
    "which gives the $(n+1)$ normal equations for the **continuous\n",
    "least squares problem**:\n",
    "\n",
    "\\begin{equation}\\label{eq:cont_normal}\n",
    "\\sum_{j=0}^{n}a_{j}\\int_{a}^{b}x^{j+k}dx=\\int_{a}^{b}f(x)x^{k}dx\n",
    "\\end{equation}\n",
    "\n",
    "for $k=0,1,...,n.$ Note that the only unknowns in these equations\n",
    "are the $a_{j}$'s; hence this is a linear system of equations. It\n",
    "is instructive to compare these normal equations with those of the\n",
    "**discrete least squares problem**:\n",
    "\\begin{equation*}\n",
    "\\sum_{j=0}^{n}a_{j}\\left(\\sum_{i=1}^{m}x_{i}^{k+j}\\right)=\\sum_{i=1}^{m}y_{i}x_{i}^{k}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909ad318",
   "metadata": {},
   "source": [
    "\\begin{example}\\label{example:chap5_exa86}\n",
    "Find the least squares polynomial approximation of degree 2 to $f(x)=e^{x}$\n",
    "on $(0,2)$.\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ff273",
   "metadata": {},
   "source": [
    "**Solution.**\n",
    "\n",
    "The normal equations are:\n",
    "\\begin{equation*}\n",
    "\\sum_{j=0}^{2}a_{j}\\int_{0}^{2}x^{j+k}dx=\\int_{0}^{2}e^{x}x^{k}dx\n",
    "\\end{equation*}\n",
    "$k=0,1,2.$ Here are the three equations:\n",
    "\\begin{align*}\n",
    "a_{0}\\int_{0}^{2}dx+a_{1}\\int_{0}^{2}xdx+a_{2}\\int_{0}^{2}x^{2}dx & =\\int_{0}^{2}e^{x}dx\\\\\n",
    "a_{0}\\int_{0}^{2}xdx+a_{1}\\int_{0}^{2}x^{2}dx+a_{2}\\int_{0}^{2}x^{3}dx & =\\int_{0}^{2}e^{x}xdx\\\\\n",
    "a_{0}\\int_{0}^{2}x^{2}dx+a_{1}\\int_{0}^{2}x^{3}dx+a_{2}\\int_{0}^{2}x^{4}dx & =\\int_{0}^{2}e^{x}x^{2}dx\n",
    "\\end{align*}\n",
    "Computing the integrals we get\n",
    "\\begin{align*}\n",
    "2a_{0}+2a_{1}+\\frac{8}{3}a_{2} & =e^{2}-1\\\\\n",
    "2a_{0}+\\frac{8}{3}a_{1}+4a_{2} & =e^{2}+1\\\\\n",
    "\\frac{8}{3}a_{0}+4a_{1}+\\frac{32}{5}a_{2} & =2e^{2}-2\n",
    "\\end{align*}\n",
    "whose solution is $a_{0}=3(-7+e^{2})$, $a_{1}=-\\frac{3}{2}(-37+5e^{2}),$\n",
    "$a_{2}=\\frac{15}{4}(-7+e^{2}).$ Then\n",
    "\\begin{equation*}\n",
    "P_{2}(x)=1.17+0.08x+1.46x^{2}.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1721aad",
   "metadata": {},
   "source": [
    "The solution method we have discussed for the least squares problem\n",
    "by solving the normal equations as a matrix equation has certain drawbacks:\n",
    "\n",
    "- The integrals $\\int_{a}^{b}x^{i+j}dx=\\left(b^{i+j+1}-a^{i+j+1}\\right)/(i+j+1)$\n",
    "in the coefficient matrix give rise to matrix equation that is prone\n",
    "to roundoff error.\n",
    "- There is no easy way to go from $P_{n}(x)$ to $P_{n+1}(x)$ (which\n",
    "we might want to do if we were not satisfied by the approximation\n",
    "provided by $P_{n}$).\n",
    "\n",
    "There is a better approach to solve the discrete and continuous least\n",
    "squares problem using the orthogonal polynomials we encountered in\n",
    "Gaussian quadrature. Both discrete and continuous least squares problem\n",
    "tries to find a polynomial $P_{n}(x)=\\sum_{j=0}^{n}a_{j}x^{j}$ that\n",
    "satisfies some properties. Notice how the polynomial is written in\n",
    "terms of the monomial basis functions $x^{j}$ and recall how these\n",
    "basis functions caused numerical difficulties in interpolation. That\n",
    "was the reason we discussed different basis functions like Lagrange\n",
    "and Newton for the interpolation problem. So the idea is to write\n",
    "$P_{n}(x)$ in terms of some other basis functions\n",
    "\\begin{equation*}\n",
    "P_{n}(x)=\\sum_{j=0}^{n}a_{j}\\phi_{j}(x)\n",
    "\\end{equation*}\n",
    "which would then update the normal equations for continuous least\n",
    "squares \\eqref{eq:cont_normal} as\n",
    "\\begin{equation*}\n",
    "\\sum_{j=0}^{n}a_{j}\\int_{a}^{b}\\phi_{j}(x)\\phi_{k}(x)dx=\\int_{a}^{b}f(x)\\phi_{k}(x)dx\n",
    "\\end{equation*}\n",
    "for $k=0,1,...,n.$ The normal equations for the discrete least squares\n",
    "gets a similar update:\n",
    "\\begin{equation*}\n",
    "\\sum_{j=0}^{n}a_{j}\\left(\\sum_{i=1}^{m}\\phi_{j}(x_{i})\\phi_{k}(x_{i})\\right)=\\sum_{i=1}^{m}y_{i}\\phi_{k}(x_{i}).\n",
    "\\end{equation*}\n",
    "Going forward, the crucial observation is that the integral of the product of two\n",
    "functions $\\int \\phi_j(x) \\phi_k(x) dx$, or the summation of the product of two functions evaluated\n",
    "at some discrete points, $\\sum \\phi_j(x_i)\\phi_k(x_i)$, can be viewed as an inner product $\\left\\langle \\phi_{j},\\phi_{k}\\right\\rangle $\n",
    "of two vectors in a suitably defined vector space. When the functions (vectors)\n",
    "$\\phi_{j}$ are **orthogonal**, the inner product $\\left\\langle \\phi_{j},\\phi_{k}\\right\\rangle $\n",
    "is 0 if $j\\neq k$, which makes the normal equations trivial to solve.\n",
    "We will discuss details in the next section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
