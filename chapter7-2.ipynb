{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe69a138",
   "metadata": {},
   "source": [
    "# 7.2 Derivative-based optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50a073",
   "metadata": {},
   "source": [
    "In this section, we introduce three derivative-based optimization methods for unconstrained optimization problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847fdc25",
   "metadata": {},
   "source": [
    "## Newton's method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5a869",
   "metadata": {},
   "source": [
    "Newton's method was introduced in Chapter 2.3 as an iterative method for solving nonlinear equations. It can also be used for numerical optimization. Consider a univariate unimodal function $f(x)$ on the interval $[a,b]$. The derivative of $f$ equals $0$ where the extremum is located. Therefore, the optimization problem $\\min{f(x)}$ or $\\max{f(x)}$ is converted to solving a nonlinear equation $f'(x)=0$. Applying Newton's method to the equation, the iterative formula is:\n",
    "\\begin{equation}\\label{eq:newton_opt}\n",
    "x_{i+1} = x_{i} - \\frac{f'(x_{i})}{f''(x_{i})}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb062c",
   "metadata": {},
   "source": [
    "The following Python function **newton_opt** performs optimization with Newton's method. The code is almost identical to the one used for solving nonlinear equations, with the input functions changed to the first and second derivatives of $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc311931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46904103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_opt(fprime, fdprime, pin, eps, N):\n",
    "    \"\"\"\n",
    "    Optimizatin using Newton's method\n",
    "    input:\n",
    "    fprime: function, the first-order derivative function\n",
    "    fdprime: function, the second-order derivative function\n",
    "    pin: float, the initial guess of the extremizer\n",
    "    eps: float, the tolerance for stopping the algorithm\n",
    "    N: int, the maximum number of steps to run\n",
    "    \"\"\"\n",
    "    n = 1\n",
    "    while n <= N:\n",
    "        p = pin - fprime(pin)/fdprime(pin)\n",
    "        if np.isclose(fprime(p), 0) or np.abs(p-pin) < eps:\n",
    "            print('p is ', p, ' and the iteration number is ', n)\n",
    "            return p\n",
    "        pin = p\n",
    "        n += 1\n",
    "    y = fprime(p)\n",
    "    print('Newton method did not converge. The last iteration gives ', \n",
    "          p, ' with derivate value ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e57a19",
   "metadata": {},
   "source": [
    "We revisit the polynomial function described in Section 7.1, and use Newton's method to find the minimum. Note that the initial guess should be sufficiently close to the extremizer; otherwise, the algorithm may not converge, or it simply converges to other extremizers that are also solutions to the equation $f'(x)=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c486ac8e",
   "metadata": {},
   "source": [
    "\\begin{example}\\label{example:newton_exa1}\n",
    "Find the minimum of the polynomial\n",
    "\\begin{equation*}\n",
    "f(x) = 3x^4+2x^2-x-1.\n",
    "\\end{equation*}\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7a9d3c",
   "metadata": {},
   "source": [
    "We already know the minimizer is close to $0.22$, so we use $x_0=0.0$ as the initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**4+2*x**2-x-1\n",
    "\n",
    "def fprime(x):\n",
    "    return 12*x**3+4*x-1\n",
    "\n",
    "def fdoubleprime(x):\n",
    "    return 36*x**2+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2286892",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "N = 200\n",
    "pin = -0.\n",
    "p = newton_opt(fprime, fdoubleprime, pin, eps, N)\n",
    "print('The function f has a minimum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b056d7",
   "metadata": {},
   "source": [
    "Newton's method can also be applied to multivariate cost functions in the form of $f(x_1,x_2,\\dots,x_d)$, where $d$ is the number of variables. The iterative formula Eqn. \\eqref{eq:newton_opt} needs to be modified, since the scalar $f'(x)$ becomes the gradient vector of $f$, \n",
    "\\begin{equation*}\n",
    "\\nabla f = \\left[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_d}\\right]^T\n",
    "\\end{equation*}\n",
    "and the scalar $f''(x)$ becomes the Hessian matrix,\n",
    "\\begin{equation*}\n",
    "H_f =\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial^2 x_1} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_d} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial^2 x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_d} \\\\\n",
    "\\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_d \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_d \\partial x_2} & \\dots & \\frac{\\partial^2 f}{\\partial^2 x_d} \\\\\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "The ratio of scalars in Eqn. \\eqref{eq:newton_opt} is now replaced by the solution of the linear system:\n",
    "\\begin{equation}\\label{eq:newton_opt_u}\n",
    "H_f\\boldsymbol{u} = \\nabla f\n",
    "\\end{equation}\n",
    "and the iterative formula for finding the extremizer of the multivariate function $f(x_1,\\dots,x_d)$ can be formulated as\n",
    "\\begin{equation}\\label{eq:newton_opt_mv}\n",
    "\\boldsymbol{x}_{i+1} = \\boldsymbol{x}_{i} - \\boldsymbol{u}.\n",
    "\\end{equation}\n",
    "Here $\\boldsymbol{u}$ comes from Eqn. \\eqref{eq:newton_opt_u}, where the Hessian matrix and the gradient vector are evaluated at the current estimate $\\boldsymbol{x}_{i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191a7c3",
   "metadata": {},
   "source": [
    "The following Python function **newton_opt_mv** performs Newton's method for optimizing multivariate cost functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8476ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_opt_mv(fgrad, fhessian, pin, eps, N):\n",
    "    '''\n",
    "    Multivariate Newton's method for optimization\n",
    "    input:\n",
    "    fgrad: function, the function that returns gradient\n",
    "    fhessian: function, the function that returns the hessian matrix\n",
    "    pin: a 1d numpy array of size d, where x=(x_1,...,x_d), the initial guess of the extremizer\n",
    "    eps: float, the tolerance for stopping the algorithm\n",
    "    N: int, the maximum number of steps to run\n",
    "    '''\n",
    "    d = pin.size # number of variables\n",
    "    n = 1\n",
    "    while n <= N:\n",
    "        p = pin + np.linalg.solve(fhessian(pin), -fgrad(pin))\n",
    "        if np.all(np.isclose(fgrad(p), np.zeros(d))) or np.linalg.norm(p-pin, ord=np.inf) < eps:\n",
    "            print('p is ', p, ' and the iteration number is ', n)\n",
    "            return p\n",
    "        pin = p.copy()\n",
    "        n += 1\n",
    "    y = fgrad(p)\n",
    "    print('Method did not converge. The last iteration gives ', \n",
    "          p, ' with gradient value ', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6755e",
   "metadata": {},
   "source": [
    "We consider the following 2D function\n",
    "\\begin{equation}\\label{eq:2Dexa}\n",
    "f(x,y) = 2x^2 + 3y^2 + x - y + 3. \n",
    "\\end{equation}\n",
    "The function has a global minimum in its domain, which can be seen from the following 2D pseudocolor and 3D surface plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670394bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0906052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2*x[0]**2+3*x[1]**2+x[0]-x[1]+3\n",
    "\n",
    "# Make a psudeocolor plot\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "x = np.linspace(-10, 10, 100)\n",
    "y = np.linspace(-10, 10, 100)\n",
    "XX, YY = np.meshgrid(x, y, indexing='ij')\n",
    "ZZ = np.zeros(XX.shape)\n",
    "for i in range(x.size):\n",
    "    for j in range(y.size):\n",
    "        ZZ[i,j] = f([XX[i,j], YY[i,j]])\n",
    "plt.pcolor(XX, YY, ZZ, shading='nearest')\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7048c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 3D surface plot\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8,8))\n",
    "surf = ax.plot_surface(XX, YY, ZZ, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "ax.set_zlabel('z', fontsize=14)\n",
    "fig.colorbar(surf, shrink=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c335c4",
   "metadata": {},
   "source": [
    "We can rewrite Eqn. \\eqref{eq:2Dexa} by completing squares as\n",
    "\\begin{equation*}\n",
    "f(x,y) = 2\\left(x+\\frac{1}{4}\\right)^2 + 3\\left(y-\\frac{1}{6}\\right)^2 + \\frac{67}{24}.\n",
    "\\end{equation*}\n",
    "So $f$ has a minimum of $\\frac{67}{24}$ at the point $\\left[-\\frac{1}{4}, \\frac{1}{6}\\right]$.\n",
    "\n",
    "Apply Newton' method to the function, with an initial vector of $[1,2]$. We can see that the same results are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3bef08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return np.array([4*x[0]+1, 6*x[1]-1])\n",
    "\n",
    "def hessian(x):\n",
    "    return np.array([[4., 0], [0, 6]])\n",
    "\n",
    "pin = np.array([1., 2])\n",
    "eps = 1e-5\n",
    "N = 200\n",
    "p = newton_opt_mv(grad, hessian, pin, eps, N)\n",
    "print('The function f has a minimum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7102602",
   "metadata": {},
   "source": [
    "We now animate the process of Newton's method. The following animation shows the true minimizer, the estimates produced by Newton's method, and the contour plot of the cost function $f(\\boldsymbol{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a0257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Newton's method for the animation\n",
    "def newton_mv_nstep(fgrad, fhessian, pin, nstep):\n",
    "    '''\n",
    "    Multivariate Newton's method for optimization. The algorithm run nstep steps\n",
    "    input:\n",
    "    fgrad: function, the function that returns gradient\n",
    "    fhessian: function, the function that returns the hessian matrix\n",
    "    pin: a 1d numpy array of size d, where x=(x_1,...,x_d), the initial guess of the extremizer\n",
    "    nstep: int, the number of steps to run\n",
    "    '''\n",
    "    d = pin.size # number of variables\n",
    "    p = pin.copy()\n",
    "    n = 1\n",
    "    while n <= nstep:\n",
    "        p = pin + np.linalg.solve(fhessian(pin), -fgrad(pin))\n",
    "        pin = p.copy()\n",
    "        n += 1\n",
    "    return p\n",
    "\n",
    "# Animation for Newton's method\n",
    "pin = np.array([1., 2])\n",
    "true_sol = np.array([-1/4, 1/6])\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = ax.contourf(XX, YY, ZZ, levels=30)\n",
    "cb = fig.colorbar(CS)\n",
    "cb.ax.set_ylabel('f(x,y)')\n",
    "def animate(i):\n",
    "    ax.cla()\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(-10, 10)  \n",
    "    ax.contourf(XX, YY, ZZ, levels=30)\n",
    "    x = newton_mv_nstep(grad, hessian, pin, i)\n",
    "    ax.plot(x[0], x[1], 'o', color='r', ms=6, label='Newton method')\n",
    "    ax.plot(true_sol[0], true_sol[1], 's', color='green', ms=6, label='True solution')\n",
    "    ax.set_title('Newton method : iteration step '+str(i), size=16)\n",
    "    ax.set_xlabel('$x$', size=16)\n",
    "    ax.set_ylabel('$y$', size=16)\n",
    "    ax.legend(loc='upper right')\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, animate, frames=20, interval=1000)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d03cad5",
   "metadata": {},
   "source": [
    "In this case, since $\\nabla f(\\boldsymbol{x})$ is linear, Newton's method converges to the true minimizer in just one step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6682412",
   "metadata": {},
   "source": [
    "\\begin{example}\\label{example:newton_opt_sincos}\n",
    "Apply Newton's method to the following function\n",
    "\\begin{equation*}\n",
    "f(x,y) = \\sin{x}\\cos{y}\n",
    "\\end{equation*}\n",
    "with different initial vectors $\\boldsymbol{x}_0$.\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158e628",
   "metadata": {},
   "source": [
    "We first visualize the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaeb5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.sin(x[0])*np.cos(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y = np.linspace(-5, 5, 100)\n",
    "XX, YY = np.meshgrid(x, y, indexing='ij')\n",
    "ZZ = np.zeros(XX.shape)\n",
    "for i in range(x.size):\n",
    "    for j in range(y.size):\n",
    "        ZZ[i,j] = f([XX[i,j], YY[i,j]])\n",
    "plt.pcolor(XX, YY, ZZ, shading='nearest')\n",
    "plt.xlabel('x', fontsize=14)\n",
    "plt.ylabel('y', fontsize=14)\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a 3D surface plot\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8,8))\n",
    "surf = ax.plot_surface(XX, YY, ZZ, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('y', fontsize=14)\n",
    "ax.set_zlabel('z', fontsize=14)\n",
    "fig.colorbar(surf, shrink=0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868b3b4d",
   "metadata": {},
   "source": [
    "The plots clearly show that there are infinitely many extrema, and minima and maxima alternate. In this case, the choice of the initial guess determines which solution Newton's method converges to. First, we consider using an initial vector of $\\boldsymbol{x}_0=[1,3]$, which is close to the point $\\left[\\frac{\\pi}{2}, \\pi\\right]$, at which $f(x)$ achieves a minimum $\\sin\\left[\\frac{\\pi}{2}\\right]\\cos{\\pi}=-1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6279c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(x):\n",
    "    return np.array([np.cos(x[0])*np.cos(x[1]), -np.sin(x[0])*np.sin(x[1])])\n",
    "\n",
    "def hessian(x):\n",
    "    return np.array([[-np.sin(x[0])*np.cos(x[1]), -np.cos(x[0])*np.sin(x[1])], \n",
    "                     [-np.cos(x[0])*np.sin(x[1]), -np.sin(x[0])*np.cos(x[1])]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1038558",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = np.array([1., 3])\n",
    "eps = 1e-5\n",
    "N = 200\n",
    "p = newton_opt_mv(grad, hessian, pin, eps, N)\n",
    "print('The function f has a minimum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a76aa",
   "metadata": {},
   "source": [
    "The results are in agreement with what we expected. Now consider another initial vector $\\boldsymbol{x}_0=[4, 3]$ and apply Newton's method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "pin = np.array([4., 3])\n",
    "eps = 1e-5\n",
    "N = 200\n",
    "p = newton_opt_mv(grad, hessian, pin, eps, N)\n",
    "print('The function f has a maximum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96737a2",
   "metadata": {},
   "source": [
    "We see that Newton's method converges to the maximum close to the initial vector.\n",
    "\n",
    "Note that Newton's method not only utilizes the first-order derivative information (gradient), but also the second-order derivatives (Hessian). If the Hessian matrix is available, Newton's method can converge faster than the two methods that only utilize the gradient information to be introduced next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83da748",
   "metadata": {},
   "source": [
    "The animated processes of the convergences of the two initial guesses are shown below. Newton's method also converges fast for both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca0155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton's method for the animation\n",
    "def newton_mv_nstep(fgrad, fhessian, pin, nstep):\n",
    "    '''\n",
    "    Multivariate Newton's method for optimization. The algorithm run nstep steps\n",
    "    input:\n",
    "    fgrad: function, the function that returns gradient\n",
    "    fhessian: function, the function that returns the hessian matrix\n",
    "    pin: a 1d numpy array of size d, where x=(x_1,...,x_d), the initial guess of the extremizer\n",
    "    nstep: int, the number of steps to run\n",
    "    '''\n",
    "    d = pin.size # number of variables\n",
    "    p = pin.copy()\n",
    "    n = 1\n",
    "    while n <= nstep:\n",
    "        p = pin + np.linalg.solve(fhessian(pin), -fgrad(pin))\n",
    "        pin = p.copy()\n",
    "        n += 1\n",
    "    return p\n",
    "\n",
    "# Animation for Newton's method\n",
    "pin1 = np.array([1., 3])\n",
    "pin2 = np.array([4., 3])\n",
    "true_sol1 = np.array([np.pi/2, np.pi])\n",
    "true_sol2 = np.array([3*np.pi/2, np.pi])\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = ax.contourf(XX, YY, ZZ, levels=30)\n",
    "cb = fig.colorbar(CS)\n",
    "cb.ax.set_ylabel('f(x,y)')\n",
    "def animate(i):\n",
    "    ax.cla()\n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)  \n",
    "    ax.contourf(XX, YY, ZZ, levels=30)\n",
    "    x1 = newton_mv_nstep(grad, hessian, pin1, i)\n",
    "    ax.plot(x1[0], x1[1], 'o', color='r', ms=6, label='Newton method-case 1')\n",
    "    ax.plot(true_sol1[0], true_sol1[1], 's', color='green', ms=6, label='True solution-case 1')\n",
    "    x2 = newton_mv_nstep(grad, hessian, pin2, i)\n",
    "    ax.plot(x2[0], x2[1], 'o', color='m', ms=6, label='Newton method-case 2')\n",
    "    ax.plot(true_sol2[0], true_sol2[1], 's', color='y', ms=6, label='True solution-case 2')\n",
    "    ax.set_title('Newton method : iteration step '+str(i), size=16)\n",
    "    ax.set_xlabel('$x$', size=16)\n",
    "    ax.set_ylabel('$y$', size=16)\n",
    "    ax.legend(loc='lower right')\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, animate, frames=20, interval=1000)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c25191",
   "metadata": {},
   "source": [
    "# Conjugate gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4d7536",
   "metadata": {},
   "source": [
    "The conjugate gradient method was discussed in Section 3.3 as an algorithm for solving linear systems  where the coefficient matrices are symmetric positive definite (SPD). Here, we show how it can be used to find extrema and extremizers. If the multivariate cost function $f(\\boldsymbol{x})$ can be written as the following quadratic form in matrix notation\n",
    "\\begin{equation}\\label{eq:quadratic_form}\n",
    "f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^TA\\boldsymbol{x} - \\boldsymbol{b}^T\\boldsymbol{x}\n",
    "\\end{equation}\n",
    "where $A$ is an $n \\times n$ SPD matrix, then the gradient of $f$ takes the form of\n",
    "\\begin{equation}\\label{eq:cg_grad}\n",
    "\\nabla f = A\\boldsymbol{x}-\\boldsymbol{b}.\n",
    "\\end{equation}\n",
    "Therefore, finding the extremizer of $f$ is equivalent to solving an SPD linear system $A\\boldsymbol{x}=\\boldsymbol{b}$, and conjugate gradient can be employed for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39585d8e",
   "metadata": {},
   "source": [
    "\\begin{example}\\label{example:cg_exa1}\n",
    "Find the minimizer of the function\n",
    "\\begin{equation*}\n",
    "f(x, y) = x^2 + y^2.\n",
    "\\end{equation*}\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a97a634",
   "metadata": {},
   "source": [
    "Note that $f$ can be written as\n",
    "\\begin{equation*}\n",
    "f(x,y) = \\frac{1}{2}\n",
    "\\begin{bmatrix}\n",
    "x & y\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "0 & 0\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "Hence in this example,\n",
    "\\begin{equation*}\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "2 & 0 \\\\\n",
    "0 & 2\n",
    "\\end{bmatrix}, \\,\\,\n",
    "\\boldsymbol{b} = \n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "The solution can be easily seen as $[0, 0]^T$, which minimizes $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d34b8",
   "metadata": {},
   "source": [
    "The following Python function **ConjGrad** performs conjugate gradient to find extremizers once the matrix $A$ and vector $\\boldsymbol{b}$ are identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88598666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjugate Gradient Method\n",
    "def ConjGrad(A, b, x0, eps):\n",
    "    \"\"\"\n",
    "    Perform conjugate Gradient method, the second version.\n",
    "    A: an symmetric positive definite matrix\n",
    "    b: the right-hand side\n",
    "    x0: the initial guess\n",
    "    eps: the tolerance for stopping the algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    if not np.array_equal(A.T, A):\n",
    "        print('Error: the matrix A is not symmetric!')\n",
    "        return\n",
    "        \n",
    "    n = A.shape[0]\n",
    "    r_old = b-np.dot(A, x0)\n",
    "    r_new = r_old.copy()\n",
    "    if np.linalg.norm(r_old) < eps:\n",
    "        return x\n",
    "    u = r_old\n",
    "    x = x0.copy()\n",
    "    \n",
    "    for k in range(n):        \n",
    "        a = np.dot(r_old, r_old)/np.dot(u, np.dot(A,u))\n",
    "        x += a*u\n",
    "        r_new = r_old - a*np.dot(A, u)\n",
    "        if np.linalg.norm(r_new) < eps:\n",
    "            return x\n",
    "        b = np.dot(r_new, r_new)/np.dot(r_old, r_old)\n",
    "        u = r_new + b*u\n",
    "        r_old = r_new.copy()\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c68913f",
   "metadata": {},
   "source": [
    "Applying ConjGrad to $f$, the same results are obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7217399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "A = np.array([[2., 0], [0, 2]])\n",
    "b = np.array([0., 0])\n",
    "x0 = np.ones(2)\n",
    "eps = 1e-9\n",
    "\n",
    "p = ConjGrad(A, b, x0, eps)\n",
    "print('The function f has a minimum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12825318",
   "metadata": {},
   "source": [
    "\\begin{example}\\label{example:cg_exa2}\n",
    "Locate the extremum of the following function of three variables:\n",
    "\\begin{equation*}\n",
    "f(x_1,x_2,x_3) = 5x_1^2 + \\frac{3}{2}x_2^2 + \\frac{3}{2}x_3^2 + 5x_1x_2 + 2x_1x_3 + 2x_2x_3 - 7x_1 - 4x_2 - 3x_3.\n",
    "\\end{equation*}\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20abb378",
   "metadata": {},
   "source": [
    "Rewrite $f(x_1,x_2,x_3)$ in the quadratic form of $\\frac{1}{2}\\boldsymbol{x}^TA\\boldsymbol{x} - \\boldsymbol{b}^T\\boldsymbol{x}$:\n",
    "\\begin{equation*}\n",
    "f(x_1,x_2,x_3) = \\frac{1}{2}\n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "10 & 5 & 2 \\\\\n",
    "5 & 3 & 2 \\\\\n",
    "2 & 2 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ x_3\n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "7 & 4 & 3\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\ x_2 \\\\ x_3\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "So $A$ and $\\boldsymbol{b}$ can be identified as\n",
    "\\begin{equation*}\n",
    "A = \n",
    "\\begin{bmatrix}\n",
    "10 & 5 & 2 \\\\\n",
    "5 & 3 & 2 \\\\\n",
    "2 & 2 & 3\n",
    "\\end{bmatrix}, \\,\\,\n",
    "\\boldsymbol{b} = \n",
    "\\begin{bmatrix}\n",
    "7 \\\\\n",
    "4 \\\\\n",
    "3\n",
    "\\end{bmatrix}.\n",
    "\\end{equation*}\n",
    "Now we can apply the conjugate gradient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb363c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x1, x2, x3 = x[0], x[1], x[2]\n",
    "    return 5*x1**2 + 3/2*x2**2 + 3/2*x3**2 + 5*x1*x2 + 2*x1*x3 + 2*x2*x3 - 7*x1 - 4*x2 - 3*x3\n",
    "\n",
    "A = np.array([[10., 5, 2], [5, 3, 2], [2, 2, 3]])\n",
    "b = np.array([7., 4, 3])\n",
    "x0 = np.zeros(3)\n",
    "eps = 1e-9\n",
    "\n",
    "p = ConjGrad(A, b, x0, eps)\n",
    "print('The function f has a extremum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412b314",
   "metadata": {},
   "source": [
    "The result shows the extremizer of $f$ is $[1, -1, 1]$ and the corresponding extremum is $-3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44837776",
   "metadata": {},
   "source": [
    "When the cost function $f(\\boldsymbol{x})$ cannot be written in the form of Eqn. \\eqref{eq:quadratic_form}, conjugate gradient can still be applied. There will be no matrix $A$ and vector $b$ and hence changes should be made to the original algorithms. In reference to the conjugate gradient algorithm of the second version in Section 3 Chapter 3, the initializations of $\\boldsymbol{r}_0$ and $\\boldsymbol{u}_0$ should be changed to\n",
    "\\begin{equation*}\n",
    "\\boldsymbol{r}_0 = -\\nabla f(\\boldsymbol{x}_0)\n",
    "\\end{equation*}\n",
    "and $\\boldsymbol{u}_0  = \\boldsymbol{r}_0 $,\n",
    "due to Eqn. \\eqref{eq:cg_grad}. In Step 2(A), the quantity $a_k$, which determines how far the current estimate should move along the new direction $\\boldsymbol{u}_k$, is obtained by minimizing the function $f(\\boldsymbol{x}_k+a\\boldsymbol{u}_k)$ with respect to $a$. This step involves a 1D minimization problem, which can be solved by e.g. golden-section search. The last part that needs to be changed is Step 2(C), where the new $\\boldsymbol{r}$ should be updated by\n",
    "\\begin{equation*}\n",
    "\\boldsymbol{r}_{k+1} = -\\nabla f(\\boldsymbol{x}_{k+1})\n",
    "\\end{equation*}\n",
    "which can be easily seen from the algorithm of the first version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f1e1e",
   "metadata": {},
   "source": [
    "Hence the algorithm of conjugate gradient method for optimization can be summarized as:\n",
    "\n",
    "1. Initialize $\\boldsymbol{x}_0$ as any vector. Set $\\boldsymbol{r}_0=-\\nabla f(\\boldsymbol{x}_0)$ and $\\boldsymbol{u}_0=\\boldsymbol{r}_0$\n",
    "\n",
    "2. For $k=0, 1, \\dots, N-1$:\n",
    "\n",
    "    1. minimize $f(\\boldsymbol{x}_k+a\\boldsymbol{u}_k)$ with respect to $a$, i.e., $a_k = \\text{argmin}_{a}f(\\boldsymbol{x}_k+a\\boldsymbol{u}_k)$\n",
    "    2. $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k + a_k \\boldsymbol{u}_k$\n",
    "    3. $\\boldsymbol{r}_{k+1} = -\\nabla f(\\boldsymbol{x}_{k+1})$\n",
    "    4. if ($||\\boldsymbol{r}_{k+1}|| < \\epsilon$):\n",
    "        1. break\n",
    "    5. $\\boldsymbol{b}_k = \\frac{\\boldsymbol{r}_{k+1}^T\\boldsymbol{r}_{k+1}}{\\boldsymbol{r}_{k}^T\\boldsymbol{r}_{k}}$\n",
    "    6. $\\boldsymbol{u}_{k+1} = \\boldsymbol{r}_{k+1} + \\boldsymbol{b}_k\\boldsymbol{u}_{k}$\n",
    "\n",
    "3. return $\\boldsymbol{x}_{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7659da",
   "metadata": {},
   "source": [
    "The Python function **ConjGrad_opt** realizes the conjugate gradient algorithm that applies to a general cost function $f(\\boldsymbol{x})$. It uses a built-in function from the *scipy* package to solve the 1d minimization problem by golden-section search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20793218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import golden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64094bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conjugate Gradient Method\n",
    "\n",
    "def ConjGrad_opt(fgrad, fa, x0, N, eps):\n",
    "    \"\"\"\n",
    "    Perform conjugate gradient search for optimization\n",
    "    fgrad: function, the gradient function of the cost function f\n",
    "    fa: function, the function f(x+au). The inputs of fa are a, x and u.\n",
    "    x0: a 1d numpy array, the initial guess\n",
    "    N: int, the maximum number of steps of CG to perform\n",
    "    eps: the tolerance for stopping the algorithm\n",
    "    \"\"\"\n",
    "        \n",
    "    r_old = -fgrad(x0)\n",
    "    r_new = r_old.copy()\n",
    "    if np.linalg.norm(r_old) < eps:\n",
    "        return x\n",
    "    u = r_old.copy()\n",
    "    x = x0.copy()\n",
    "    \n",
    "    for k in range(N):        \n",
    "        a = golden(fa, args=(x, u))\n",
    "        x += a*u\n",
    "        r_new = -fgrad(x)\n",
    "        if np.linalg.norm(r_new) < eps:\n",
    "            return x\n",
    "        b = np.dot(r_new, r_new)/np.dot(r_old, r_old)\n",
    "        u = r_new + b*u\n",
    "        r_old = r_new.copy()\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91e20be",
   "metadata": {},
   "source": [
    "\\begin{example}\\label{example:cg_exa3}\n",
    "Locate the minimum of the following function using conjugate gradient method.\n",
    "\\begin{equation*}\n",
    "f(x,y) = \\frac{1}{2}x^2 - 2x + y^2 - 2y + 6.\n",
    "\\end{equation*}\n",
    "\\end{example}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e5e8f",
   "metadata": {},
   "source": [
    "Completing squares for the cost function, we can easily see that the minimizer is $[2,1]$ and the corresponding minimum is $3$. Now we apply the *ConjGrad_opt* Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a66870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.5*x[0]**2-2*x[0]+x[1]**2-2*x[1]+6\n",
    "def fgrad(x):\n",
    "    return np.array([x[0]-2, 2*x[1]-2])\n",
    "def fa(a, x, u):\n",
    "    return f(x+a*u)\n",
    "\n",
    "x0 = np.zeros(2)\n",
    "N = 200\n",
    "eps = 1e-9\n",
    "p = ConjGrad_opt(fgrad, fa, x0, N, eps)\n",
    "print('The function f has a minimum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576e97ee",
   "metadata": {},
   "source": [
    "The following animation demonstrates the convergence of conjugate gradient, along with the $\\boldsymbol{r}$ and $\\boldsymbol{u}$ directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b473e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 7, 100)\n",
    "y = np.linspace(-4, 6, 100)\n",
    "XX, YY = np.meshgrid(x, y, indexing='ij')\n",
    "ZZ = np.zeros(XX.shape)\n",
    "for i in range(x.size):\n",
    "    for j in range(y.size):\n",
    "        ZZ[i,j] = f([XX[i,j], YY[i,j]])\n",
    "        \n",
    "# Conjugate gradient method for the animation\n",
    "def ConjGrad_opt_nstep(fgrad, fa, x0, nstep):\n",
    "    \"\"\"\n",
    "    Perform conjugate Gradient search for optimization for nstep steps\n",
    "    fgrad: function, the gradient function of the cost function f\n",
    "    fa: function, the function f(x+au). The inputs of fa are a, x and u.\n",
    "    x0: a 1d numpy array, the initial guess\n",
    "    nstep: int, the number of steps of CG to perform\n",
    "    \"\"\"\n",
    "        \n",
    "    r_old = -fgrad(x0)\n",
    "    r_new = r_old.copy()\n",
    "    if np.linalg.norm(r_old) < eps:\n",
    "        return x\n",
    "    u = r_old.copy()\n",
    "    x = x0.copy()\n",
    "    \n",
    "    for k in range(nstep):        \n",
    "        a = golden(fa, args=(x, u))\n",
    "        x += a*u\n",
    "        r_new = -fgrad(x)\n",
    "        b = np.dot(r_new, r_new)/np.dot(r_old, r_old)\n",
    "        u = r_new + b*u\n",
    "        r_old = r_new.copy()\n",
    "    a  = golden(fa, args=(x, u))   \n",
    "    return x, r_new, u, a\n",
    "\n",
    "# Animation for Newton's method\n",
    "x0 = np.zeros(2)\n",
    "true_sol = np.array([2., 1])\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = ax.contourf(XX, YY, ZZ, levels=30)\n",
    "cb = fig.colorbar(CS)\n",
    "cb.ax.set_ylabel('f(x,y)')\n",
    "def animate(i):\n",
    "    ax.cla()\n",
    "    ax.set_xlim(-3, 7)\n",
    "    ax.set_ylim(-4, 6)  \n",
    "    ax.contourf(XX, YY, ZZ, levels=30)\n",
    "    x, r, u, a = ConjGrad_opt_nstep(fgrad, fa, x0, i)\n",
    "    x1 = newton_mv_nstep(grad, hessian, pin1, i)\n",
    "    ax.plot(x[0], x[1], 'o', color='r', ms=6, label='CG')\n",
    "    ax.quiver([x[0]], [x[1]], [a*u[0]], [a*u[1]], angles='xy', scale_units='xy', scale=1, \n",
    "              color='r', label='u direction')\n",
    "    ax.quiver([x[0]], [x[1]], [r[0]], [r[1]], angles='xy', \n",
    "              color='c', label='r direction')\n",
    "    ax.plot(true_sol[0], true_sol[1], 's', color='green', ms=6, label='True solution')\n",
    "    ax.set_title('Conjugate gradient method : iteration step '+str(i), size=16)\n",
    "    ax.set_xlabel('$x$', size=16)\n",
    "    ax.set_ylabel('$y$', size=16)\n",
    "    ax.legend(loc='lower right')\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, animate, frames=3, interval=1000)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de7a0d",
   "metadata": {},
   "source": [
    "It can be seen that conjugate gradient locates the minimizer after 2 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f9c30e",
   "metadata": {},
   "source": [
    "# Steepest descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c2112",
   "metadata": {},
   "source": [
    "As the last method to be introduced employing the derivative information to locate minimizers of a function $f(\\boldsymbol{x})$, steepest descent searches the minimum by constantly moving along the negative gradient direction, along which the value of the cost function drops the fastest. The algorithm determines how far the estimate $\\boldsymbol{x}$ moves by solving a 1d minimization problem similar to the conjugate gradient method. \n",
    "\n",
    "The steepest descent algorithm can be summarized as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b3a4c2",
   "metadata": {},
   "source": [
    "1. Initialize $\\boldsymbol{x}_0$.\n",
    "\n",
    "2. For $k=0, 1, \\dots, N-1$:\n",
    "\n",
    "    1. find the gradient $\\boldsymbol{g} = \\nabla f(\\boldsymbol{x}_k)$\n",
    "    2. minimize $f(\\boldsymbol{x}_k-a\\boldsymbol{g})$ with respect to $a$. Let $a_k=\\text{argmin}_{a}f(\\boldsymbol{x}_k-a\\boldsymbol{g})$\n",
    "    3. update $\\boldsymbol{x}$ by $\\boldsymbol{x}_{k+1} = \\boldsymbol{x}_k-a_k\\boldsymbol{g}$\n",
    "\n",
    "3. return $\\boldsymbol{x}_{k+1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2853ae",
   "metadata": {},
   "source": [
    "The following Python function **steepest_descent** implements the steepest descent algorithm. The 1d minimization problem is again solved by using golden-section search. The algorithm stops either when a specified maximum number of steps is reached or when the norm of the gradient is small enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaab87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steepest Descent Method\n",
    "\n",
    "def steepest_descent(fgrad, fa, x0, N, eps):\n",
    "    \"\"\"\n",
    "    Perform steepest descent search for optimization\n",
    "    fgrad: function, the gradient function of the cost function f\n",
    "    fa: function, the function f(x-ag). The inputs of fa are a, x and g.\n",
    "    x0: a 1d numpy array, the initial guess\n",
    "    N: int, the maximum number of steps of SD to perform\n",
    "    eps: the tolerance for stopping the algorithm\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    for k in range(N):    \n",
    "        g = fgrad(x)\n",
    "        if np.linalg.norm(g) < eps:\n",
    "            return x\n",
    "        a = golden(fa, args=(x, g))\n",
    "        x -= a*g\n",
    "                \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462419a2",
   "metadata": {},
   "source": [
    "\\begin{example}\\label{example:sd_exa1}\n",
    "Revisit Example \\ref{example:cg_exa3} using steepest descent.\n",
    "\\end{example}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b549907",
   "metadata": {},
   "source": [
    "Apply the steepest descent algorithm with the *steepest_descent* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ad493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.5*x[0]**2-2*x[0]+x[1]**2-2*x[1]+6\n",
    "\n",
    "def fgrad(x):\n",
    "    return np.array([x[0]-2, 2*x[1]-2])\n",
    "\n",
    "def fa(a, x, g):\n",
    "    return f(x-a*g)\n",
    "\n",
    "x0 = np.zeros(2)\n",
    "N = 200\n",
    "eps = 1e-9\n",
    "p = steepest_descent(fgrad, fa, x0, N, eps)\n",
    "print('The function f has a minimum of ', f(p), ' at ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1b219",
   "metadata": {},
   "source": [
    "Note that the convergence of steepest descent is significantly slower than that of the conjugate gradient method, although both produce the correct solution. We demonstrate the slow convergence using the following animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c05430",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 7, 100)\n",
    "y = np.linspace(-4, 6, 100)\n",
    "XX, YY = np.meshgrid(x, y, indexing='ij')\n",
    "ZZ = np.zeros(XX.shape)\n",
    "for i in range(x.size):\n",
    "    for j in range(y.size):\n",
    "        ZZ[i,j] = f([XX[i,j], YY[i,j]])\n",
    "\n",
    "# Newton's method for the animation\n",
    "def steepest_descent_nstep(fgrad, fa, x0, nstep):\n",
    "    \"\"\"\n",
    "    Perform steepest descent search for optimization for nstep steps\n",
    "    fgrad: function, the gradient function of the cost function f\n",
    "    fa: function, the function f(x-ag). The inputs of fa are a, x and g.\n",
    "    x0: a 1d numpy array, the initial guess\n",
    "    nstep: int, the number of steps of SD to perform\n",
    "    \"\"\"\n",
    "    x = x0.copy()\n",
    "    for k in range(nstep):    \n",
    "        g = fgrad(x)\n",
    "        a = golden(fa, args=(x, g))\n",
    "        x -= a*g\n",
    "    g = fgrad(x)\n",
    "    a = golden(fa, args=(x, g))            \n",
    "    return x, a, g\n",
    "\n",
    "# Animation for Newton's method\n",
    "x0 = np.zeros(2)\n",
    "true_sol = np.array([2., 1])\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "CS = ax.contourf(XX, YY, ZZ, levels=30)\n",
    "cb = fig.colorbar(CS)\n",
    "cb.ax.set_ylabel('f(x,y)')\n",
    "def animate(i):\n",
    "    ax.cla()\n",
    "    ax.set_xlim(-3, 7)\n",
    "    ax.set_ylim(-4, 6)  \n",
    "    ax.contourf(XX, YY, ZZ, levels=30)\n",
    "    x, a, g = steepest_descent_nstep(fgrad, fa, x0, i)\n",
    "    ax.plot(x[0], x[1], 'o', color='r', ms=6, label='Steepest descent method')\n",
    "    ax.quiver([x[0]], [x[1]], [-a*g[0]], [-a*g[1]], angles='xy', scale_units='xy', scale=1, \n",
    "              color='m', label='Negative gradient direction')\n",
    "    ax.plot(true_sol[0], true_sol[1], 's', color='green', ms=6, label='True solution')\n",
    "    ax.set_title('Steepest descent : iteration step '+str(i), size=16)\n",
    "    ax.set_xlabel('$x$', size=16)\n",
    "    ax.set_ylabel('$y$', size=16)\n",
    "    ax.legend(loc='upper right')\n",
    "plt.close()\n",
    "anim = FuncAnimation(fig, animate, frames=20, interval=1000)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb7471",
   "metadata": {},
   "source": [
    "\\begin{exercise}\\label{ex:ex1}\n",
    "Consider the Rosenbrock function\n",
    "\\begin{equation*}\n",
    "f(x,y) = (a-x)^2 + b(y-x^2)^2\n",
    "\\end{equation*}\n",
    "where $a=1$ and $b=100$. Apply Newton's method, conjugate gradient and steepest descent to the function to find its minimizer.\n",
    "\\end{exercise}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6d3052",
   "metadata": {},
   "source": [
    "\\begin{exercise}\\label{ex:ex2}\n",
    "Consider the Beale function\n",
    "\\begin{equation*}\n",
    "f(x,y) = (1.5-x+xy)^2 + (2.25-x+xy^2)^2 + (2.625-x+xy^3)^2\n",
    "\\end{equation*}\n",
    "defined on $[-4.5, 4.5]^2$.\n",
    "Apply Newton's method, conjugate gradient and steepest descent to the function to find its minimizer.\n",
    "\\end{exercise}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09abcaf",
   "metadata": {},
   "source": [
    "\\begin{exercise}\\label{ex:ex3}\n",
    "Show that Newton's method applied to a cost function of the following form,\n",
    "\\begin{equation*}\n",
    "f(x,y) = ax^2 + bx^2 + cx + dy + e\n",
    "\\end{equation*}\n",
    "where $a, b, c, d, e\\in\\mathbb{R}$, converges in just one step.\n",
    "\\end{exercise}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
